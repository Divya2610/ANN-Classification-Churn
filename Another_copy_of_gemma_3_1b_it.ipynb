{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "gpu"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5a232874b88043149a352c5ffa3d0e80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_aea482c32b5a4a75a9b3fd031e321f72"
          }
        },
        "c1ae79d31cd24cb29ea2b19789ce7d00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a9dd3f1939648d69b0bae7ae2658828",
            "placeholder": "​",
            "style": "IPY_MODEL_b187cad2a18d4cd8805701314af747cc",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "b17cb33c1a5946babc64cba42421d3cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_854e1df960e34bbf9b81f615e142dd7e",
            "placeholder": "​",
            "style": "IPY_MODEL_7b94c7c319bc4e6e9bb4e3708c6bc83c",
            "value": ""
          }
        },
        "e81dd213cdde47ae963e18333aa11412": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_78336060208749a7ba2b6f8cb65bf284",
            "style": "IPY_MODEL_d609eff4b0a44564b999310a1d122413",
            "value": true
          }
        },
        "f782dee1edfe464aa841d6793aa28d34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_ebf3557ccd1841c197a8955f6dc45a4c",
            "style": "IPY_MODEL_1baf49c098794522a89ae10d741333db",
            "tooltip": ""
          }
        },
        "0467bcdfd65f4a698edc1406a7dcc4dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_893c421b4ec24dbcaa7169882cb8fbef",
            "placeholder": "​",
            "style": "IPY_MODEL_1877d5755326418ca3ce9df611fdc520",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "aea482c32b5a4a75a9b3fd031e321f72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "3a9dd3f1939648d69b0bae7ae2658828": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b187cad2a18d4cd8805701314af747cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "854e1df960e34bbf9b81f615e142dd7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b94c7c319bc4e6e9bb4e3708c6bc83c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78336060208749a7ba2b6f8cb65bf284": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d609eff4b0a44564b999310a1d122413": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ebf3557ccd1841c197a8955f6dc45a4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1baf49c098794522a89ae10d741333db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "893c421b4ec24dbcaa7169882cb8fbef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1877d5755326418ca3ce9df611fdc520": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb96a2b6be1e41a5a68fe8c80343587f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51492e49f7284887af06b8ddc2156d7f",
            "placeholder": "​",
            "style": "IPY_MODEL_2df8ea9771a14b9aa31366c7cdda8929",
            "value": "Connecting..."
          }
        },
        "51492e49f7284887af06b8ddc2156d7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2df8ea9771a14b9aa31366c7cdda8929": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4dec36e60f9f4885be47b6fe5391bd94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_892f27c37720403c854e5e023d992f47",
              "IPY_MODEL_ccf208d564ad4fa5aea64ec4ee219e15",
              "IPY_MODEL_2b5a049d98f34bb8a975cf30c36fee68"
            ],
            "layout": "IPY_MODEL_42f4f458cd9a42b2bde31e6e5793f8a3"
          }
        },
        "892f27c37720403c854e5e023d992f47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fc19272f8ac4702b44cf09202a68e4d",
            "placeholder": "​",
            "style": "IPY_MODEL_ed0e56f3a56d4aecb05813c1fa05e454",
            "value": "config.json: 100%"
          }
        },
        "ccf208d564ad4fa5aea64ec4ee219e15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98e4d980f6c349c08e523e51b02d4ca1",
            "max": 899,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7f040a3470db4705a76e51b391db4ad2",
            "value": 899
          }
        },
        "2b5a049d98f34bb8a975cf30c36fee68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80882affc88a4fa4a771f1e123fc28fc",
            "placeholder": "​",
            "style": "IPY_MODEL_712c0e33cca64661898190aa0596f94a",
            "value": " 899/899 [00:00&lt;00:00, 80.1kB/s]"
          }
        },
        "42f4f458cd9a42b2bde31e6e5793f8a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fc19272f8ac4702b44cf09202a68e4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed0e56f3a56d4aecb05813c1fa05e454": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98e4d980f6c349c08e523e51b02d4ca1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f040a3470db4705a76e51b391db4ad2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "80882affc88a4fa4a771f1e123fc28fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "712c0e33cca64661898190aa0596f94a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e654cba55439415eae8e9a933cdca378": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4d846217b37c46c1a4a8daed9bbc3d87",
              "IPY_MODEL_ac52174001d94984acb776eeb76ea322",
              "IPY_MODEL_4fd23fd130fc4c209ed8f4a31a8e693e"
            ],
            "layout": "IPY_MODEL_7b9bc47d210446f7979c171b2f2fe2b8"
          }
        },
        "4d846217b37c46c1a4a8daed9bbc3d87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_433ec4dccc084123a06e68b53aa1727a",
            "placeholder": "​",
            "style": "IPY_MODEL_fc33b02558b641aa8a27b240356620c2",
            "value": "model.safetensors: 100%"
          }
        },
        "ac52174001d94984acb776eeb76ea322": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fbd0f5b2a4f48919dc56da0f0b0dabe",
            "max": 1999811208,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_74e909a31a3d4b47bd9d1f8d3ed97e38",
            "value": 1999811208
          }
        },
        "4fd23fd130fc4c209ed8f4a31a8e693e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5ffbb0ff1934e67aef081ba9b5c47ab",
            "placeholder": "​",
            "style": "IPY_MODEL_08b500dd4bd446209fc7e78b5fcef546",
            "value": " 2.00G/2.00G [00:18&lt;00:00, 77.6MB/s]"
          }
        },
        "7b9bc47d210446f7979c171b2f2fe2b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "433ec4dccc084123a06e68b53aa1727a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc33b02558b641aa8a27b240356620c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2fbd0f5b2a4f48919dc56da0f0b0dabe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74e909a31a3d4b47bd9d1f8d3ed97e38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b5ffbb0ff1934e67aef081ba9b5c47ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08b500dd4bd446209fc7e78b5fcef546": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1358d2b73ccb464990fef33370f39748": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d7a672f637cd4a3b9e01a9e281417ddb",
              "IPY_MODEL_5e47297f30744b7a871bc97d292d5dad",
              "IPY_MODEL_250ced7dec5547b4b0d2b94c1995c46c"
            ],
            "layout": "IPY_MODEL_2de8390ba49a451690e53b98435ac8eb"
          }
        },
        "d7a672f637cd4a3b9e01a9e281417ddb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8966daf99f544e7a9c3ff07879a92a6f",
            "placeholder": "​",
            "style": "IPY_MODEL_549cc3683cf54b5ab2a55475e9c02d61",
            "value": "generation_config.json: 100%"
          }
        },
        "5e47297f30744b7a871bc97d292d5dad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8e293f497e9498d823a29e6593d0d17",
            "max": 215,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_834a47b80a764276acbaa9325a27cc43",
            "value": 215
          }
        },
        "250ced7dec5547b4b0d2b94c1995c46c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9ce0fed67a54f4c9566c370b7fa4d39",
            "placeholder": "​",
            "style": "IPY_MODEL_1c24d71cef2c4eab97ea6b8823b5ea04",
            "value": " 215/215 [00:00&lt;00:00, 14.8kB/s]"
          }
        },
        "2de8390ba49a451690e53b98435ac8eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8966daf99f544e7a9c3ff07879a92a6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "549cc3683cf54b5ab2a55475e9c02d61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a8e293f497e9498d823a29e6593d0d17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "834a47b80a764276acbaa9325a27cc43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c9ce0fed67a54f4c9566c370b7fa4d39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c24d71cef2c4eab97ea6b8823b5ea04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9c3d8ff923845088229c1318a7c9341": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b57ee7313b9843959c8524380a13f71d",
              "IPY_MODEL_46f7a0da570f4f7ca1fc83a0a9a9251f",
              "IPY_MODEL_1ccdf503cf184bc495bc141fd8e66802"
            ],
            "layout": "IPY_MODEL_bfe50d1bba984de8bfcfdf946f0b5ab6"
          }
        },
        "b57ee7313b9843959c8524380a13f71d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78e01bbde2204aa0abadb63dcd20c961",
            "placeholder": "​",
            "style": "IPY_MODEL_fa84ccabe0cf4400ae492236ca3e4c83",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "46f7a0da570f4f7ca1fc83a0a9a9251f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2bc1b732c9a426ca8c4e290311707d2",
            "max": 1156999,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_edd368da9276413286e97191aa523a6e",
            "value": 1156999
          }
        },
        "1ccdf503cf184bc495bc141fd8e66802": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d68948351584b6ca2b0a371ddac11eb",
            "placeholder": "​",
            "style": "IPY_MODEL_541d2eb2e6ee4daaa4c2beba7a7ec131",
            "value": " 1.16M/1.16M [00:00&lt;00:00, 12.7MB/s]"
          }
        },
        "bfe50d1bba984de8bfcfdf946f0b5ab6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78e01bbde2204aa0abadb63dcd20c961": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa84ccabe0cf4400ae492236ca3e4c83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2bc1b732c9a426ca8c4e290311707d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edd368da9276413286e97191aa523a6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7d68948351584b6ca2b0a371ddac11eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "541d2eb2e6ee4daaa4c2beba7a7ec131": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "99df14f327154fc6b9da3d4e30abbd43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4964c5b2dda948e58d8489a54637b34b",
              "IPY_MODEL_6c549e582d184fad938c0d8b723e11c1",
              "IPY_MODEL_61f9eb827f944eb8aab08a1d17be1345"
            ],
            "layout": "IPY_MODEL_4256f4c591d74f66acdb405fe5346dc9"
          }
        },
        "4964c5b2dda948e58d8489a54637b34b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ff95d7918a3459cb5492f4bf6a9bd52",
            "placeholder": "​",
            "style": "IPY_MODEL_78753ef7c36b407c9926c7521cea0153",
            "value": "tokenizer.model: 100%"
          }
        },
        "6c549e582d184fad938c0d8b723e11c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32165cc609fa4b3ea9b5a20d4f61617e",
            "max": 4689074,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a52227740d854d9e95091c90eb4bf291",
            "value": 4689074
          }
        },
        "61f9eb827f944eb8aab08a1d17be1345": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbf94a32f43f4b5596ac8deb4225bf7a",
            "placeholder": "​",
            "style": "IPY_MODEL_5ce1d5a2d5aa416ab517b7c287a6bcc2",
            "value": " 4.69M/4.69M [00:00&lt;00:00, 8.07MB/s]"
          }
        },
        "4256f4c591d74f66acdb405fe5346dc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ff95d7918a3459cb5492f4bf6a9bd52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78753ef7c36b407c9926c7521cea0153": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32165cc609fa4b3ea9b5a20d4f61617e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a52227740d854d9e95091c90eb4bf291": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fbf94a32f43f4b5596ac8deb4225bf7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ce1d5a2d5aa416ab517b7c287a6bcc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5bc4425895604377996255e6d8403c47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cd60de35572a46019d5c809fafaae4ab",
              "IPY_MODEL_11acac4348cd41069498e1af21c73cd3",
              "IPY_MODEL_4addbd0ccab440df8b73694adadf05fb"
            ],
            "layout": "IPY_MODEL_ac11cab8b219435a9537f328b087a6b4"
          }
        },
        "cd60de35572a46019d5c809fafaae4ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_abb981a263984169b4e92c452852ae91",
            "placeholder": "​",
            "style": "IPY_MODEL_89314d4183c14c0bae5e5f29f7695618",
            "value": "tokenizer.json: 100%"
          }
        },
        "11acac4348cd41069498e1af21c73cd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a76d22336f7348dd823a645485ef385a",
            "max": 33384568,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1570c451ca8344b5bf7781a328468119",
            "value": 33384568
          }
        },
        "4addbd0ccab440df8b73694adadf05fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e058c6d10887484f941aeae77a3ad1a1",
            "placeholder": "​",
            "style": "IPY_MODEL_14b77a455e074b31b48efd0327b42010",
            "value": " 33.4M/33.4M [00:00&lt;00:00, 42.6MB/s]"
          }
        },
        "ac11cab8b219435a9537f328b087a6b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abb981a263984169b4e92c452852ae91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89314d4183c14c0bae5e5f29f7695618": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a76d22336f7348dd823a645485ef385a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1570c451ca8344b5bf7781a328468119": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e058c6d10887484f941aeae77a3ad1a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14b77a455e074b31b48efd0327b42010": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de505be4eafe4925b028183b86ae6c73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4840ecf808d1485fbb11f961cb330a7c",
              "IPY_MODEL_85ba5ca1a77a4c6889f6ac83c4852731",
              "IPY_MODEL_3c30cf69bc5a406383a696dfbab861c9"
            ],
            "layout": "IPY_MODEL_f0bf60473d4e4c23b723a04217415340"
          }
        },
        "4840ecf808d1485fbb11f961cb330a7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b24a3d9c023422ebbd0866d838a4f89",
            "placeholder": "​",
            "style": "IPY_MODEL_02cd5338345a4542b44f154933ba4c01",
            "value": "added_tokens.json: 100%"
          }
        },
        "85ba5ca1a77a4c6889f6ac83c4852731": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf70e8030c364cae817dbee3c7cb194b",
            "max": 35,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4f330e9e6421417d8ff235e8aaa34374",
            "value": 35
          }
        },
        "3c30cf69bc5a406383a696dfbab861c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b50779b86a60431c9ffa88318b7d295b",
            "placeholder": "​",
            "style": "IPY_MODEL_47b427394fe14f0a8d8abe0eec515e3a",
            "value": " 35.0/35.0 [00:00&lt;00:00, 4.26kB/s]"
          }
        },
        "f0bf60473d4e4c23b723a04217415340": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b24a3d9c023422ebbd0866d838a4f89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02cd5338345a4542b44f154933ba4c01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf70e8030c364cae817dbee3c7cb194b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f330e9e6421417d8ff235e8aaa34374": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b50779b86a60431c9ffa88318b7d295b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47b427394fe14f0a8d8abe0eec515e3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6862a7f17480425895761ef9605ea794": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4c9cb2b979e44cd89bd340a7586d4bf9",
              "IPY_MODEL_80231d22d51b48179b9c6536daea503b",
              "IPY_MODEL_4ce97b5ac68c401d9e1d23d5f07f1bd6"
            ],
            "layout": "IPY_MODEL_32a684f582924dd4b753e2cbcdc9cbfd"
          }
        },
        "4c9cb2b979e44cd89bd340a7586d4bf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_480872e7879f4482a6fa4a8a24a773eb",
            "placeholder": "​",
            "style": "IPY_MODEL_ba59d79ee86f4f0a879567bee9779026",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "80231d22d51b48179b9c6536daea503b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b292474510b04ce799697baa7d02e316",
            "max": 662,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b08a28031a664e97b6f51578808eeb6e",
            "value": 662
          }
        },
        "4ce97b5ac68c401d9e1d23d5f07f1bd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fac4495e34964a66b834004e8d89e86e",
            "placeholder": "​",
            "style": "IPY_MODEL_a6d9f7e61b684de081341b49ffbdc205",
            "value": " 662/662 [00:00&lt;00:00, 88.9kB/s]"
          }
        },
        "32a684f582924dd4b753e2cbcdc9cbfd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "480872e7879f4482a6fa4a8a24a773eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba59d79ee86f4f0a879567bee9779026": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b292474510b04ce799697baa7d02e316": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b08a28031a664e97b6f51578808eeb6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fac4495e34964a66b834004e8d89e86e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6d9f7e61b684de081341b49ffbdc205": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Divya2610/ANN-Classification-Churn/blob/main/Another_copy_of_gemma_3_1b_it.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oanVvprxx0c",
        "outputId": "2029cf04-7bef-4d1d-c76d-599108e65930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Local Inference on GPU\n",
        "Model page: https://huggingface.co/google/gemma-3-1b-it\n",
        "\n",
        "⚠️ If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/google/gemma-3-1b-it)\n",
        "\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) 🙏"
      ],
      "metadata": {
        "id": "BDuk-i_Nxx0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model you are trying to use is gated. Please make sure you have access to it by visiting the model page.To run inference, either set HF_TOKEN in your environment variables/ Secrets or run the following cell to login. 🤗"
      ],
      "metadata": {
        "id": "Se8P0Mdvxx0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fastapi uvicorn transformers torch sentence-transformers faiss-cpu accelerate\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERh-QE51512J",
        "outputId": "bca8e608-669b-41d4-9664-11b95316be20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (0.123.10)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (0.38.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.50.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from fastapi) (2.12.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (4.15.0)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.0.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (8.3.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.2)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.51.0,>=0.40.0->fastapi) (4.12.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m105.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.13.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(new_session=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "5a232874b88043149a352c5ffa3d0e80",
            "c1ae79d31cd24cb29ea2b19789ce7d00",
            "b17cb33c1a5946babc64cba42421d3cb",
            "e81dd213cdde47ae963e18333aa11412",
            "f782dee1edfe464aa841d6793aa28d34",
            "0467bcdfd65f4a698edc1406a7dcc4dd",
            "aea482c32b5a4a75a9b3fd031e321f72",
            "3a9dd3f1939648d69b0bae7ae2658828",
            "b187cad2a18d4cd8805701314af747cc",
            "854e1df960e34bbf9b81f615e142dd7e",
            "7b94c7c319bc4e6e9bb4e3708c6bc83c",
            "78336060208749a7ba2b6f8cb65bf284",
            "d609eff4b0a44564b999310a1d122413",
            "ebf3557ccd1841c197a8955f6dc45a4c",
            "1baf49c098794522a89ae10d741333db",
            "893c421b4ec24dbcaa7169882cb8fbef",
            "1877d5755326418ca3ce9df611fdc520",
            "eb96a2b6be1e41a5a68fe8c80343587f",
            "51492e49f7284887af06b8ddc2156d7f",
            "2df8ea9771a14b9aa31366c7cdda8929"
          ]
        },
        "id": "mFW5dOw0xx0e",
        "outputId": "f4a290d1-e0ba-4933-f477-219f41b5b366"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a232874b88043149a352c5ffa3d0e80"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"google/gemma-3-1b-it\")\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "pipe(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377,
          "referenced_widgets": [
            "4dec36e60f9f4885be47b6fe5391bd94",
            "892f27c37720403c854e5e023d992f47",
            "ccf208d564ad4fa5aea64ec4ee219e15",
            "2b5a049d98f34bb8a975cf30c36fee68",
            "42f4f458cd9a42b2bde31e6e5793f8a3",
            "2fc19272f8ac4702b44cf09202a68e4d",
            "ed0e56f3a56d4aecb05813c1fa05e454",
            "98e4d980f6c349c08e523e51b02d4ca1",
            "7f040a3470db4705a76e51b391db4ad2",
            "80882affc88a4fa4a771f1e123fc28fc",
            "712c0e33cca64661898190aa0596f94a",
            "e654cba55439415eae8e9a933cdca378",
            "4d846217b37c46c1a4a8daed9bbc3d87",
            "ac52174001d94984acb776eeb76ea322",
            "4fd23fd130fc4c209ed8f4a31a8e693e",
            "7b9bc47d210446f7979c171b2f2fe2b8",
            "433ec4dccc084123a06e68b53aa1727a",
            "fc33b02558b641aa8a27b240356620c2",
            "2fbd0f5b2a4f48919dc56da0f0b0dabe",
            "74e909a31a3d4b47bd9d1f8d3ed97e38",
            "b5ffbb0ff1934e67aef081ba9b5c47ab",
            "08b500dd4bd446209fc7e78b5fcef546",
            "1358d2b73ccb464990fef33370f39748",
            "d7a672f637cd4a3b9e01a9e281417ddb",
            "5e47297f30744b7a871bc97d292d5dad",
            "250ced7dec5547b4b0d2b94c1995c46c",
            "2de8390ba49a451690e53b98435ac8eb",
            "8966daf99f544e7a9c3ff07879a92a6f",
            "549cc3683cf54b5ab2a55475e9c02d61",
            "a8e293f497e9498d823a29e6593d0d17",
            "834a47b80a764276acbaa9325a27cc43",
            "c9ce0fed67a54f4c9566c370b7fa4d39",
            "1c24d71cef2c4eab97ea6b8823b5ea04",
            "e9c3d8ff923845088229c1318a7c9341",
            "b57ee7313b9843959c8524380a13f71d",
            "46f7a0da570f4f7ca1fc83a0a9a9251f",
            "1ccdf503cf184bc495bc141fd8e66802",
            "bfe50d1bba984de8bfcfdf946f0b5ab6",
            "78e01bbde2204aa0abadb63dcd20c961",
            "fa84ccabe0cf4400ae492236ca3e4c83",
            "d2bc1b732c9a426ca8c4e290311707d2",
            "edd368da9276413286e97191aa523a6e",
            "7d68948351584b6ca2b0a371ddac11eb",
            "541d2eb2e6ee4daaa4c2beba7a7ec131",
            "99df14f327154fc6b9da3d4e30abbd43",
            "4964c5b2dda948e58d8489a54637b34b",
            "6c549e582d184fad938c0d8b723e11c1",
            "61f9eb827f944eb8aab08a1d17be1345",
            "4256f4c591d74f66acdb405fe5346dc9",
            "0ff95d7918a3459cb5492f4bf6a9bd52",
            "78753ef7c36b407c9926c7521cea0153",
            "32165cc609fa4b3ea9b5a20d4f61617e",
            "a52227740d854d9e95091c90eb4bf291",
            "fbf94a32f43f4b5596ac8deb4225bf7a",
            "5ce1d5a2d5aa416ab517b7c287a6bcc2",
            "5bc4425895604377996255e6d8403c47",
            "cd60de35572a46019d5c809fafaae4ab",
            "11acac4348cd41069498e1af21c73cd3",
            "4addbd0ccab440df8b73694adadf05fb",
            "ac11cab8b219435a9537f328b087a6b4",
            "abb981a263984169b4e92c452852ae91",
            "89314d4183c14c0bae5e5f29f7695618",
            "a76d22336f7348dd823a645485ef385a",
            "1570c451ca8344b5bf7781a328468119",
            "e058c6d10887484f941aeae77a3ad1a1",
            "14b77a455e074b31b48efd0327b42010",
            "de505be4eafe4925b028183b86ae6c73",
            "4840ecf808d1485fbb11f961cb330a7c",
            "85ba5ca1a77a4c6889f6ac83c4852731",
            "3c30cf69bc5a406383a696dfbab861c9",
            "f0bf60473d4e4c23b723a04217415340",
            "6b24a3d9c023422ebbd0866d838a4f89",
            "02cd5338345a4542b44f154933ba4c01",
            "bf70e8030c364cae817dbee3c7cb194b",
            "4f330e9e6421417d8ff235e8aaa34374",
            "b50779b86a60431c9ffa88318b7d295b",
            "47b427394fe14f0a8d8abe0eec515e3a",
            "6862a7f17480425895761ef9605ea794",
            "4c9cb2b979e44cd89bd340a7586d4bf9",
            "80231d22d51b48179b9c6536daea503b",
            "4ce97b5ac68c401d9e1d23d5f07f1bd6",
            "32a684f582924dd4b753e2cbcdc9cbfd",
            "480872e7879f4482a6fa4a8a24a773eb",
            "ba59d79ee86f4f0a879567bee9779026",
            "b292474510b04ce799697baa7d02e316",
            "b08a28031a664e97b6f51578808eeb6e",
            "fac4495e34964a66b834004e8d89e86e",
            "a6d9f7e61b684de081341b49ffbdc205"
          ]
        },
        "id": "SgpzMu3hxx0f",
        "outputId": "fc2830e2-6a5f-40de-e778-9798086eede8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/899 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4dec36e60f9f4885be47b6fe5391bd94"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e654cba55439415eae8e9a933cdca378"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1358d2b73ccb464990fef33370f39748"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e9c3d8ff923845088229c1318a7c9341"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "99df14f327154fc6b9da3d4e30abbd43"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5bc4425895604377996255e6d8403c47"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de505be4eafe4925b028183b86ae6c73"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6862a7f17480425895761ef9605ea794"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': [{'role': 'user', 'content': 'Who are you?'},\n",
              "   {'role': 'assistant',\n",
              "    'content': 'Hi there! I’m Gemma, a large language model created by the Gemma team at Google DeepMind. I’m an open-weights model, which means I’m publicly available for anyone to use! \\n\\nI’m designed to take text and images as input and produce text as output. \\n\\nIt’s nice to meet you! 😊'}]}]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-1b-it\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-1b-it\")\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "\tmessages,\n",
        "\tadd_generation_prompt=True,\n",
        "\ttokenize=True,\n",
        "\treturn_dict=True,\n",
        "\treturn_tensors=\"pt\",\n",
        ").to(model.device)\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=40)\n",
        "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJBcO0ojxx0f",
        "outputId": "1b711304-ac9a-4866-9e7c-7d0d78b1cade"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi there! I’m Gemma, a large language model created by the Gemma team at Google DeepMind. I’m an open-weights model, which means I’m publicly available for use\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-1b-it\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"google/gemma-3-1b-it\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ").eval()\n",
        "\n",
        "class Prompt(BaseModel):\n",
        "    text: str\n",
        "\n",
        "@app.post(\"/generate\")\n",
        "def generate(prompt: Prompt):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt.text}]\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        outputs = model.generate(inputs, max_new_tokens=80)\n",
        "\n",
        "    result = tokenizer.decode(\n",
        "        outputs[0][inputs.shape[-1]:],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "    return {\"response\": result}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXDt46yL0JMq",
        "outputId": "8739debf-402c-473d-c5ed-af48fcfff576"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4kywErC56-I",
        "outputId": "75f23314-b9a9-4160-a0a7-803830c881da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m⚠️  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `hf auth whoami` to get more information or `hf auth logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): Traceback (most recent call last):\n",
            "object address  : 0x7fc857918280\n",
            "object refcount : 3\n",
            "object type     : 0xa2a4e0\n",
            "object type name: KeyboardInterrupt\n",
            "object repr     : KeyboardInterrupt()\n",
            "lost sys.stderr\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mkdir llm_api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rK3Ev8156QwB",
        "outputId": "ae44c890-b729-40cf-b4fa-63a96a6616c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘llm_api’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd llm_api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkVehQq-6R_Y",
        "outputId": "7198008b-eea8-4c8e-f115-078952f8c74d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/llm_api\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!touch app.py"
      ],
      "metadata": {
        "id": "euAH0Mfk6WA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IKSfFBZYobO",
        "outputId": "5a8ede36-fec7-49b8-d701-501968b620a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "app.py\tdrive  llm_api\tsample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/llm_api/app.py\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "MODEL_NAME = \"google/gemma-3-1b-it\"\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "tokenizer = None\n",
        "model = None\n",
        "\n",
        "class Prompt(BaseModel):\n",
        "    text: str\n",
        "\n",
        "@app.get(\"/\")\n",
        "def root():\n",
        "    return {\"status\": \"ok\", \"message\": \"LLM API is running\"}\n",
        "\n",
        "@app.on_event(\"startup\")\n",
        "def load_model():\n",
        "    global tokenizer, model\n",
        "    print(\"🔄 Loading model...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        dtype=torch.float16,      # 🔧 small fix (torch_dtype → dtype)\n",
        "        device_map=\"auto\",\n",
        "    ).eval()\n",
        "    print(\"✅ Model loaded\")\n",
        "\n",
        "@app.post(\"/generate\")\n",
        "def generate(prompt: Prompt):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt.text}]\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(model.device)\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        outputs = model.generate(inputs, max_new_tokens=80)\n",
        "\n",
        "    result = tokenizer.decode(\n",
        "        outputs[0][inputs.shape[-1]:],\n",
        "        skip_special_tokens=True,\n",
        "    )\n",
        "    return {\"response\": result}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxgP9Xc7Yv-u",
        "outputId": "f6e54437-6a1a-4f34-9d23-a9df2236f3b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/llm_api/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -n '1,50p' /content/llm_api/app.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "me4UcNF1bun-",
        "outputId": "2d10b83f-d4b4-4a33-b3fd-30c873fb0127"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "from fastapi import FastAPI\n",
            "from pydantic import BaseModel\n",
            "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
            "import torch\n",
            "\n",
            "MODEL_NAME = \"google/gemma-3-1b-it\"\n",
            "\n",
            "app = FastAPI()\n",
            "\n",
            "tokenizer = None\n",
            "model = None\n",
            "\n",
            "class Prompt(BaseModel):\n",
            "    text: str\n",
            "\n",
            "@app.get(\"/\")\n",
            "def root():\n",
            "    return {\"status\": \"ok\", \"message\": \"LLM API is running\"}\n",
            "\n",
            "@app.on_event(\"startup\")\n",
            "def load_model():\n",
            "    global tokenizer, model\n",
            "    print(\"🔄 Loading model...\")\n",
            "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
            "    model = AutoModelForCausalLM.from_pretrained(\n",
            "        MODEL_NAME,\n",
            "        dtype=torch.float16,      # 🔧 small fix (torch_dtype → dtype)\n",
            "        device_map=\"auto\",\n",
            "    ).eval()\n",
            "    print(\"✅ Model loaded\")\n",
            "\n",
            "@app.post(\"/generate\")\n",
            "def generate(prompt: Prompt):\n",
            "    messages = [{\"role\": \"user\", \"content\": prompt.text}]\n",
            "    inputs = tokenizer.apply_chat_template(\n",
            "        messages,\n",
            "        add_generation_prompt=True,\n",
            "        return_tensors=\"pt\",\n",
            "    ).to(model.device)\n",
            "\n",
            "    with torch.inference_mode():\n",
            "        outputs = model.generate(inputs, max_new_tokens=80)\n",
            "\n",
            "    result = tokenizer.decode(\n",
            "        outputs[0][inputs.shape[-1]:],\n",
            "        skip_special_tokens=True,\n",
            "    )\n",
            "    return {\"response\": result}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/llm_api\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MhyxhpNbw99",
        "outputId": "14bf98da-48e0-44cd-e8c7-fd347e24bbee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "app.py\t__init__.py  __pycache__\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "os.chdir('/content') # Change the current directory back to /content\n",
        "\n",
        "# Remove the module from sys.modules to force a reload\n",
        "if 'llm_api.app' in sys.modules:\n",
        "    del sys.modules['llm_api.app']\n",
        "if 'llm_api' in sys.modules:\n",
        "    del sys.modules['llm_api']\n",
        "\n",
        "from llm_api.app import app\n",
        "print(type(app))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6799v8Dbz9k",
        "outputId": "7933759b-e266-4b3d-e22a-ef122915cfc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'fastapi.applications.FastAPI'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup python -m uvicorn llm_api.app:app --host 0.0.0.0 --port 8000 > uvicorn.log 2>&1 &"
      ],
      "metadata": {
        "id": "HXRIjgfufKG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "requests.get(\"http://localhost:8000/\").json()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0J_6BgMgqx8",
        "outputId": "66ebc355-9a70-44d1-d7be-007f591ead65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'message': 'LLM API is running'}"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74e7d370"
      },
      "source": [
        "First, let's stop any processes currently listening on port 8000 to ensure a clean restart. We'll use `lsof` to find the process ID (PID) and `kill` to terminate it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fd8b076"
      },
      "source": [
        "!kill $(lsof -t -i:8000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b8a975e"
      },
      "source": [
        "Now, let's restart the Uvicorn server with the updated `app.py` which includes the new root endpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d7871f6"
      },
      "source": [
        "!nohup python -m uvicorn llm_api.app:app --host 0.0.0.0 --port 8000 > uvicorn.log 2>&1 &"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85dc05e2"
      },
      "source": [
        "Finally, let's try calling the root endpoint again to verify the changes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e35cfe8a",
        "outputId": "4d1b0208-83a5-416a-89b0-27dcb65e0ca3"
      },
      "source": [
        "import requests\n",
        "import time\n",
        "\n",
        "time.sleep(30) # Wait for 30 seconds to allow the server and model to fully start\n",
        "requests.get(\"http://localhost:8000/\").json()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'message': 'LLM API is running'}"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd4bea20",
        "outputId": "5dc9a234-7a80-4a25-d2b0-b12c36fdbcc2"
      },
      "source": [
        "!cat uvicorn.log"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     Started server process [26559]\n",
            "INFO:     Waiting for application startup.\n",
            "2026-01-12 09:36:43.012893: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1768210603.048919   26559 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1768210603.060557   26559 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1768210603.086881   26559 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768210603.086909   26559 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768210603.086913   26559 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768210603.086916   26559 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1722254c"
      },
      "source": [
        "Now that the server is running, let's test the text generation endpoint by sending a POST request with a sample prompt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33a43db7",
        "outputId": "fc444336-c62c-4a3a-a713-4131429dbb99"
      },
      "source": [
        "import requests\n",
        "\n",
        "response = requests.post(\n",
        "    \"http://localhost:8000/generate\",\n",
        "    json={\"text\": \"What is the capital of France?\"}\n",
        ")\n",
        "print(response.json())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'response': 'The capital of France is **Paris**. \\n\\nWould you like to know more about Paris? For example, I could tell you about its history, landmarks, or things to do there!'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2de0e171"
      },
      "source": [
        "Now that the server is running, let's test the text generation endpoint by sending a POST request with a sample prompt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ef4d4b5",
        "outputId": "5695d274-526c-44af-d2cc-a8115470b049"
      },
      "source": [
        "import requests\n",
        "\n",
        "response = requests.post(\n",
        "    \"http://localhost:8000/generate\",\n",
        "    json={\"text\": \"What is the capital of Japan?\"}\n",
        ")\n",
        "print(response.json())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'response': 'The capital of Japan is **Tokyo**.\\n'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25a91615"
      },
      "source": [
        "# Task\n",
        "Enhance the `app.py` FastAPI application to support Retrieval Augmented Generation (RAG). This involves installing necessary RAG dependencies, implementing functions for loading and processing documents into chunks, generating vector embeddings for these chunks using `sentence-transformers`, and building a persistent FAISS vector store for efficient retrieval."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "483934a6"
      },
      "source": [
        "## Prepare RAG Dependencies\n",
        "\n",
        "### Subtask:\n",
        "Ensure all necessary Python libraries for RAG (Retrieval Augmented Generation) are installed, such as `langchain` or similar frameworks, alongside the existing `transformers`, `sentence-transformers`, and `faiss-cpu`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e64ab6f"
      },
      "source": [
        "**Reasoning**:\n",
        "Install `langchain` as specified in the instructions to ensure all necessary libraries for RAG are present.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98f70be8",
        "outputId": "f79b5c4c-a383-4577-ce2f-1e8bb925de9a"
      },
      "source": [
        "pip install langchain"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.2.1)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.5)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.4.59)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.12.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.3.0)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.1->langchain) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.1)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (2.32.4)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (0.25.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (4.12.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (2.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b47bd559"
      },
      "source": [
        "**Reasoning**:\n",
        "Install `tiktoken` as specified in the instructions to ensure all necessary libraries for RAG are present, especially for Langchain's tokenization needs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5eea964",
        "outputId": "d1ce0c93-d715-4363-f360-48ed8c3c1d83"
      },
      "source": [
        "pip install tiktoken"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "136946d4"
      },
      "source": [
        "## Implement Document Processing\n",
        "\n",
        "### Subtask:\n",
        "Develop Python functions within the `app.py` that can load a given document, split it into manageable chunks, and generate vector embeddings for each chunk using `sentence-transformers`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99fb67ce"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires modifying the `app.py` file to include RAG functionalities. This step involves adding necessary imports, initializing the `SentenceTransformer` model, and defining functions for loading/chunking documents and generating embeddings, all within the `app.py` file. I will use `%%writefile` to update the file with the new code.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39382f36",
        "outputId": "456a6385-8be2-41f7-9c7c-e9f7e82f6c11"
      },
      "source": [
        "%%writefile /content/llm_api/app.py\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "MODEL_NAME = \"google/gemma-3-1b-it\"\n",
        "EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "tokenizer = None\n",
        "model = None\n",
        "embedding_model = None\n",
        "\n",
        "class Prompt(BaseModel):\n",
        "    text: str\n",
        "\n",
        "@app.get(\"/\")\n",
        "def root():\n",
        "    return {\"status\": \"ok\", \"message\": \"LLM API is running\"}\n",
        "\n",
        "@app.on_event(\"startup\")\n",
        "def load_model():\n",
        "    global tokenizer, model, embedding_model\n",
        "    print(\" Loading models...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "    ).eval()\n",
        "    embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
        "    print(\" Models loaded\")\n",
        "\n",
        "def load_and_chunk_document(file_path: str):\n",
        "    loader = TextLoader(file_path)\n",
        "    documents = loader.load()\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "    return chunks\n",
        "\n",
        "def generate_embeddings(chunks):\n",
        "    texts = [doc.page_content for doc in chunks]\n",
        "    embeddings = embedding_model.encode(texts, convert_to_tensor=True)\n",
        "    return embeddings\n",
        "\n",
        "@app.post(\"/generate\")\n",
        "def generate(prompt: Prompt):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt.text}]\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(model.device)\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        outputs = model.generate(inputs, max_new_tokens=80)\n",
        "\n",
        "    result = tokenizer.decode(\n",
        "        outputs[0][inputs.shape[-1]:],\n",
        "        skip_special_tokens=True,\n",
        "    )\n",
        "    return {\"response\": result}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/llm_api/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0e8d93a"
      },
      "source": [
        "## Build and Persist FAISS Vector Store\n",
        "\n",
        "### Subtask:\n",
        "Create a FAISS (Facebook AI Similarity Search) index from the generated document embeddings. This index will serve as our searchable knowledge base. Implement functionality to save and load this index for persistence.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4768fec0"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires modifying the `app.py` file to include FAISS vector store functionalities. This step involves adding necessary imports, defining constants, updating the `load_model` function to properly initialize `embedding_model` for Langchain's FAISS, adding a global `vector_store` variable, and implementing `create_and_save_vector_store` and `load_vector_store` functions, along with calling `load_vector_store` during startup. I will use `%%writefile` to update the file with the new code.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d808c3a9",
        "outputId": "99b4f9e8-cfeb-4dd2-d09d-11280a719844"
      },
      "source": [
        "%%writefile /content/llm_api/app.py\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
        "import os\n",
        "\n",
        "MODEL_NAME = \"google/gemma-3-1b-it\"\n",
        "EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
        "FAISS_INDEX_PATH = \"faiss_index\"\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "tokenizer = None\n",
        "model = None\n",
        "embedding_model_langchain = None # Changed name to avoid conflict and signify Langchain wrapper\n",
        "vector_store = None\n",
        "\n",
        "class Prompt(BaseModel):\n",
        "    text: str\n",
        "\n",
        "@app.get(\"/\")\n",
        "def root():\n",
        "    return {\"status\": \"ok\", \"message\": \"LLM API is running\"}\n",
        "\n",
        "@app.on_event(\"startup\")\n",
        "def load_model():\n",
        "    global tokenizer, model, embedding_model_langchain, vector_store\n",
        "    print(\"🔄 Loading models...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "    ).eval()\n",
        "\n",
        "    # Initialize SentenceTransformer and wrap it for Langchain compatibility\n",
        "    st_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
        "    embedding_model_langchain = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={'device': 'cuda'}) # Using 'cuda' if available\n",
        "    print(\"✅ Models loaded\")\n",
        "\n",
        "    # Attempt to load existing vector store on startup\n",
        "    load_vector_store()\n",
        "\n",
        "def load_and_chunk_document(file_path: str):\n",
        "    loader = TextLoader(file_path)\n",
        "    documents = loader.load()\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "    return chunks\n",
        "\n",
        "def create_and_save_vector_store(chunks):\n",
        "    global vector_store\n",
        "    print(\"Building and saving FAISS index...\")\n",
        "    vector_store = FAISS.from_documents(chunks, embedding_model_langchain)\n",
        "    vector_store.save_local(FAISS_INDEX_PATH)\n",
        "    print(f\"FAISS index saved to {FAISS_INDEX_PATH}\")\n",
        "\n",
        "def load_vector_store():\n",
        "    global vector_store\n",
        "    if os.path.exists(FAISS_INDEX_PATH):\n",
        "        print(f\"Loading FAISS index from {FAISS_INDEX_PATH}...\")\n",
        "        # allow_dangerous_deserialization is required for loading FAISS index\n",
        "        vector_store = FAISS.load_local(FAISS_INDEX_PATH, embedding_model_langchain, allow_dangerous_deserialization=True)\n",
        "        print(\"FAISS index loaded.\")\n",
        "    else:\n",
        "        print(\"No existing FAISS index found. A new one will be created if needed.\")\n",
        "\n",
        "@app.post(\"/generate\")\n",
        "def generate(prompt: Prompt):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt.text}]\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(model.device)\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        outputs = model.generate(inputs, max_new_tokens=80)\n",
        "\n",
        "    result = tokenizer.decode(\n",
        "        outputs[0][inputs.shape[-1]:],\n",
        "        skip_special_tokens=True,\n",
        "    )\n",
        "    return {\"response\": result}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/llm_api/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5aba7bf"
      },
      "source": [
        "## Develop Retrieval Mechanism\n",
        "\n",
        "### Subtask:\n",
        "Write a function to perform a similarity search on the FAISS index given a user query, retrieving the most relevant document chunks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad149939"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires adding a `retrieve_documents` function to `app.py` that performs a similarity search on the `vector_store`. I will use `%%writefile` to update the file, ensuring the new function is included.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "175c949f",
        "outputId": "2170890d-6eff-4d0b-d43f-24fd76c9d838"
      },
      "source": [
        "%%writefile /content/llm_api/app.py\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
        "import os\n",
        "\n",
        "MODEL_NAME = \"google/gemma-3-1b-it\"\n",
        "EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
        "FAISS_INDEX_PATH = \"faiss_index\"\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "tokenizer = None\n",
        "model = None\n",
        "embedding_model_langchain = None\n",
        "vector_store = None\n",
        "\n",
        "class Prompt(BaseModel):\n",
        "    text: str\n",
        "\n",
        "@app.get(\"/\")\n",
        "def root():\n",
        "    return {\"status\": \"ok\", \"message\": \"LLM API is running\"}\n",
        "\n",
        "@app.on_event(\"startup\")\n",
        "def load_model():\n",
        "    global tokenizer, model, embedding_model_langchain, vector_store\n",
        "    print(\"🔄 Loading models...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "    ).eval()\n",
        "\n",
        "    # Initialize SentenceTransformer and wrap it for Langchain compatibility\n",
        "    # st_model = SentenceTransformer(EMBEDDING_MODEL_NAME) # This line is not needed if using SentenceTransformerEmbeddings\n",
        "    embedding_model_langchain = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={'device': 'cuda'})\n",
        "    print(\"✅ Models loaded\")\n",
        "\n",
        "    # Attempt to load existing vector store on startup\n",
        "    load_vector_store()\n",
        "\n",
        "def load_and_chunk_document(file_path: str):\n",
        "    loader = TextLoader(file_path)\n",
        "    documents = loader.load()\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "    return chunks\n",
        "\n",
        "def create_and_save_vector_store(chunks):\n",
        "    global vector_store\n",
        "    print(\"Building and saving FAISS index...\")\n",
        "    vector_store = FAISS.from_documents(chunks, embedding_model_langchain)\n",
        "    vector_store.save_local(FAISS_INDEX_PATH)\n",
        "    print(f\"FAISS index saved to {FAISS_INDEX_PATH}\")\n",
        "\n",
        "def load_vector_store():\n",
        "    global vector_store\n",
        "    if os.path.exists(FAISS_INDEX_PATH):\n",
        "        print(f\"Loading FAISS index from {FAISS_INDEX_PATH}...\")\n",
        "        vector_store = FAISS.load_local(FAISS_INDEX_PATH, embedding_model_langchain, allow_dangerous_deserialization=True)\n",
        "        print(\"FAISS index loaded.\")\n",
        "    else:\n",
        "        print(\"No existing FAISS index found. A new one will be created if needed.\")\n",
        "\n",
        "def retrieve_documents(query: str, k: int = 4):\n",
        "    \"\"\"Performs a similarity search on the FAISS index to retrieve relevant document chunks.\"\"\"\n",
        "    if vector_store is None:\n",
        "        print(\"Vector store not initialized. Cannot retrieve documents.\")\n",
        "        return []\n",
        "    print(f\"Retrieving top {k} documents for query: '{query}'\")\n",
        "    docs = vector_store.similarity_search(query, k=k)\n",
        "    return docs\n",
        "\n",
        "@app.post(\"/generate\")\n",
        "def generate(prompt: Prompt):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt.text}]\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(model.device)\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        outputs = model.generate(inputs, max_new_tokens=80)\n",
        "\n",
        "    result = tokenizer.decode(\n",
        "        outputs[0][inputs.shape[-1]:],\n",
        "        skip_special_tokens=True,\n",
        "    )\n",
        "    return {\"response\": result}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/llm_api/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a994bfe"
      },
      "source": [
        "## Integrate RAG with Language Model\n",
        "\n",
        "### Subtask:\n",
        "Modify the text generation logic to incorporate the retrieved document chunks as context for the language model. This will enable the model to answer questions based on the document content and generate summaries.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9938311a"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to update the `generate` function in `app.py` to implement the RAG logic as specified, including an optional `context_enabled` parameter, retrieving documents, formatting context, and constructing a new prompt. This requires overwriting the existing `app.py` file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "056d979b",
        "outputId": "ecc1c804-99fc-47f2-e61f-e11bd4a98308"
      },
      "source": [
        "%%writefile /content/llm_api/app.py\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
        "import os\n",
        "\n",
        "MODEL_NAME = \"google/gemma-3-1b-it\"\n",
        "EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
        "FAISS_INDEX_PATH = \"faiss_index\"\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "tokenizer = None\n",
        "model = None\n",
        "embedding_model_langchain = None\n",
        "vector_store = None\n",
        "\n",
        "class Prompt(BaseModel):\n",
        "    text: str\n",
        "    context_enabled: bool = False # New parameter\n",
        "\n",
        "@app.get(\"/\")\n",
        "def root():\n",
        "    return {\"status\": \"ok\", \"message\": \"LLM API is running\"}\n",
        "\n",
        "@app.on_event(\"startup\")\n",
        "def load_model():\n",
        "    global tokenizer, model, embedding_model_langchain, vector_store\n",
        "    print(\"🔄 Loading models...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "    ).eval()\n",
        "\n",
        "    embedding_model_langchain = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={'device': 'cuda'})\n",
        "    print(\"✅ Models loaded\")\n",
        "\n",
        "    load_vector_store()\n",
        "\n",
        "def load_and_chunk_document(file_path: str):\n",
        "    loader = TextLoader(file_path)\n",
        "    documents = loader.load()\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "    return chunks\n",
        "\n",
        "def create_and_save_vector_store(chunks):\n",
        "    global vector_store\n",
        "    print(\"Building and saving FAISS index...\")\n",
        "    vector_store = FAISS.from_documents(chunks, embedding_model_langchain)\n",
        "    vector_store.save_local(FAISS_INDEX_PATH)\n",
        "    print(f\"FAISS index saved to {FAISS_INDEX_PATH}\")\n",
        "\n",
        "def load_vector_store():\n",
        "    global vector_store\n",
        "    if os.path.exists(FAISS_INDEX_PATH):\n",
        "        print(f\"Loading FAISS index from {FAISS_INDEX_PATH}...\")\n",
        "        vector_store = FAISS.load_local(FAISS_INDEX_PATH, embedding_model_langchain, allow_dangerous_deserialization=True)\n",
        "        print(\"FAISS index loaded.\")\n",
        "    else:\n",
        "        print(\"No existing FAISS index found. A new one will be created if needed.\")\n",
        "\n",
        "def retrieve_documents(query: str, k: int = 4):\n",
        "    if vector_store is None:\n",
        "        print(\"Vector store not initialized. Cannot retrieve documents.\")\n",
        "        return []\n",
        "    print(f\"Retrieving top {k} documents for query: '{query}'\")\n",
        "    docs = vector_store.similarity_search(query, k=k)\n",
        "    return docs\n",
        "\n",
        "@app.post(\"/generate\")\n",
        "def generate(prompt: Prompt):\n",
        "    user_query = prompt.text\n",
        "    if prompt.context_enabled:\n",
        "        # Retrieve relevant documents\n",
        "        retrieved_docs = retrieve_documents(user_query)\n",
        "        if retrieved_docs:\n",
        "            # Format retrieved documents into a context string\n",
        "            context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "            # Construct a new prompt with context\n",
        "            messages = [\n",
        "                {\"role\": \"user\", \"content\": f\"Based on the following information:\\n\\n{context}\\n\\nAnswer the question: {user_query}\"}\n",
        "            ]\n",
        "        else:\n",
        "            messages = [{\"role\": \"user\", \"content\": user_query}]\n",
        "    else:\n",
        "        messages = [{\"role\": \"user\", \"content\": user_query}]\n",
        "\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(model.device)\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        outputs = model.generate(inputs, max_new_tokens=80)\n",
        "\n",
        "    result = tokenizer.decode(\n",
        "        outputs[0][inputs.shape[-1]:],\n",
        "        skip_special_tokens=True,\n",
        "    )\n",
        "    return {\"response\": result}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/llm_api/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92fac642"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that `app.py` has been updated with the RAG logic, I will display the first 50 lines of the file to verify the changes, especially the `generate` function and the new `context_enabled` parameter in the `Prompt` class.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bbbc81b",
        "outputId": "b5b84e69-36de-4734-926b-c9d35d646ff7"
      },
      "source": [
        "%%bash\n",
        "sed -n '1,100p' /content/llm_api/app.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "from fastapi import FastAPI\n",
            "from pydantic import BaseModel\n",
            "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
            "import torch\n",
            "from langchain_community.document_loaders import TextLoader\n",
            "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
            "from sentence_transformers import SentenceTransformer\n",
            "from langchain_community.vectorstores import FAISS\n",
            "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
            "import os\n",
            "\n",
            "MODEL_NAME = \"google/gemma-3-1b-it\"\n",
            "EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
            "FAISS_INDEX_PATH = \"faiss_index\"\n",
            "\n",
            "app = FastAPI()\n",
            "\n",
            "tokenizer = None\n",
            "model = None\n",
            "embedding_model_langchain = None\n",
            "vector_store = None\n",
            "\n",
            "class Prompt(BaseModel):\n",
            "    text: str\n",
            "    context_enabled: bool = False # New parameter\n",
            "\n",
            "@app.get(\"/\")\n",
            "def root():\n",
            "    return {\"status\": \"ok\", \"message\": \"LLM API is running\"}\n",
            "\n",
            "@app.on_event(\"startup\")\n",
            "def load_model():\n",
            "    global tokenizer, model, embedding_model_langchain, vector_store\n",
            "    print(\"🔄 Loading models...\")\n",
            "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
            "    model = AutoModelForCausalLM.from_pretrained(\n",
            "        MODEL_NAME,\n",
            "        dtype=torch.float16,\n",
            "        device_map=\"auto\",\n",
            "    ).eval()\n",
            "\n",
            "    embedding_model_langchain = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={'device': 'cuda'})\n",
            "    print(\"✅ Models loaded\")\n",
            "\n",
            "    load_vector_store()\n",
            "\n",
            "def load_and_chunk_document(file_path: str):\n",
            "    loader = TextLoader(file_path)\n",
            "    documents = loader.load()\n",
            "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
            "    chunks = text_splitter.split_documents(documents)\n",
            "    return chunks\n",
            "\n",
            "def create_and_save_vector_store(chunks):\n",
            "    global vector_store\n",
            "    print(\"Building and saving FAISS index...\")\n",
            "    vector_store = FAISS.from_documents(chunks, embedding_model_langchain)\n",
            "    vector_store.save_local(FAISS_INDEX_PATH)\n",
            "    print(f\"FAISS index saved to {FAISS_INDEX_PATH}\")\n",
            "\n",
            "def load_vector_store():\n",
            "    global vector_store\n",
            "    if os.path.exists(FAISS_INDEX_PATH):\n",
            "        print(f\"Loading FAISS index from {FAISS_INDEX_PATH}...\")\n",
            "        vector_store = FAISS.load_local(FAISS_INDEX_PATH, embedding_model_langchain, allow_dangerous_deserialization=True)\n",
            "        print(\"FAISS index loaded.\")\n",
            "    else:\n",
            "        print(\"No existing FAISS index found. A new one will be created if needed.\")\n",
            "\n",
            "def retrieve_documents(query: str, k: int = 4):\n",
            "    if vector_store is None:\n",
            "        print(\"Vector store not initialized. Cannot retrieve documents.\")\n",
            "        return []\n",
            "    print(f\"Retrieving top {k} documents for query: '{query}'\")\n",
            "    docs = vector_store.similarity_search(query, k=k)\n",
            "    return docs\n",
            "\n",
            "@app.post(\"/generate\")\n",
            "def generate(prompt: Prompt):\n",
            "    user_query = prompt.text\n",
            "    if prompt.context_enabled:\n",
            "        # Retrieve relevant documents\n",
            "        retrieved_docs = retrieve_documents(user_query)\n",
            "        if retrieved_docs:\n",
            "            # Format retrieved documents into a context string\n",
            "            context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
            "            # Construct a new prompt with context\n",
            "            messages = [\n",
            "                {\"role\": \"user\", \"content\": f\"Based on the following information:\\n\\n{context}\\n\\nAnswer the question: {user_query}\"}\n",
            "            ]\n",
            "        else:\n",
            "            messages = [{\"role\": \"user\", \"content\": user_query}]\n",
            "    else:\n",
            "        messages = [{\"role\": \"user\", \"content\": user_query}]\n",
            "\n",
            "    inputs = tokenizer.apply_chat_template(\n",
            "        messages,\n",
            "        add_generation_prompt=True,\n",
            "        return_tensors=\"pt\",\n",
            "    ).to(model.device)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8ddcf8d"
      },
      "source": [
        "## Update FastAPI for RAG Endpoints\n",
        "\n",
        "### Subtask:\n",
        "Add new endpoints to the FastAPI application (in `app.py`) for: 1) uploading and processing a document (which will trigger vector store creation), 2) requesting a summary of the loaded document, and 3) asking questions related to the document.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0126f94"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to update the `app.py` file to include the new RAG endpoints for document upload, RAG-based questioning, and summarization, as well as the necessary imports. I will use `%%writefile` to overwrite the entire file with the updated content.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2f81a38",
        "outputId": "fb731ef0-b218-46e2-cf40-b37476cba4d1"
      },
      "source": [
        "%%writefile /content/llm_api/app.py\n",
        "from fastapi import FastAPI, UploadFile, File\n",
        "from pydantic import BaseModel\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "MODEL_NAME = \"google/gemma-3-1b-it\"\n",
        "EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
        "FAISS_INDEX_PATH = \"faiss_index\"\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "tokenizer = None\n",
        "model = None\n",
        "embedding_model_langchain = None\n",
        "vector_store = None\n",
        "\n",
        "class Prompt(BaseModel):\n",
        "    text: str\n",
        "\n",
        "@app.get(\"/\")\n",
        "def root():\n",
        "    return {\"status\": \"ok\", \"message\": \"LLM API is running\"}\n",
        "\n",
        "@app.on_event(\"startup\")\n",
        "def load_model():\n",
        "    global tokenizer, model, embedding_model_langchain, vector_store\n",
        "    print(\"🔄 Loading models...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "    ).eval()\n",
        "\n",
        "    embedding_model_langchain = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={'device': 'cuda'})\n",
        "    print(\"✅ Models loaded\")\n",
        "\n",
        "    load_vector_store()\n",
        "\n",
        "def load_and_chunk_document(file_path: str):\n",
        "    loader = TextLoader(file_path)\n",
        "    documents = loader.load()\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "    return chunks\n",
        "\n",
        "def create_and_save_vector_store(chunks):\n",
        "    global vector_store\n",
        "    print(\"Building and saving FAISS index...\")\n",
        "    vector_store = FAISS.from_documents(chunks, embedding_model_langchain)\n",
        "    vector_store.save_local(FAISS_INDEX_PATH)\n",
        "    print(f\"FAISS index saved to {FAISS_INDEX_PATH}\")\n",
        "\n",
        "def load_vector_store():\n",
        "    global vector_store\n",
        "    if os.path.exists(FAISS_INDEX_PATH):\n",
        "        print(f\"Loading FAISS index from {FAISS_INDEX_PATH}...\")\n",
        "        vector_store = FAISS.load_local(FAISS_INDEX_PATH, embedding_model_langchain, allow_dangerous_deserialization=True)\n",
        "        print(\"FAISS index loaded.\")\n",
        "    else:\n",
        "        print(\"No existing FAISS index found. A new one will be created if needed.\")\n",
        "\n",
        "def retrieve_documents(query: str, k: int = 4):\n",
        "    if vector_store is None:\n",
        "        print(\"Vector store not initialized. Cannot retrieve documents.\")\n",
        "        return []\n",
        "    print(f\"Retrieving top {k} documents for query: '{query}'\")\n",
        "    docs = vector_store.similarity_search(query, k=k)\n",
        "    return docs\n",
        "\n",
        "@app.post(\"/upload-document\")\n",
        "async def upload_document(file: UploadFile = File(...)):\n",
        "    try:\n",
        "        # Create a temporary file to store the uploaded content\n",
        "        temp_file_path = f\"/tmp/{file.filename}\"\n",
        "        with open(temp_file_path, \"wb\") as buffer:\n",
        "            shutil.copyfileobj(file.file, buffer)\n",
        "\n",
        "        # Process the document and create the vector store\n",
        "        chunks = load_and_chunk_document(temp_file_path)\n",
        "        create_and_save_vector_store(chunks)\n",
        "\n",
        "        return {\"message\": f\"Document '{file.filename}' processed and vector store built successfully.\"}\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "    finally:\n",
        "        # Clean up the temporary file\n",
        "        if os.path.exists(temp_file_path):\n",
        "            os.remove(temp_file_path)\n",
        "\n",
        "@app.post(\"/ask-rag\")\n",
        "def ask_rag(prompt: Prompt):\n",
        "    user_query = prompt.text\n",
        "    retrieved_docs = retrieve_documents(user_query)\n",
        "\n",
        "    if retrieved_docs:\n",
        "        context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": f\"Based on the following information:\\n\\n{context}\\n\\nAnswer the question: {user_query}\"}\n",
        "        ]\n",
        "    else:\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": f\"I could not find relevant information in the knowledge base. Please try rephrasing or asking a general question: {user_query}\"}\n",
        "        ]\n",
        "\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(model.device)\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        outputs = model.generate(inputs, max_new_tokens=80)\n",
        "\n",
        "    result = tokenizer.decode(\n",
        "        outputs[0][inputs.shape[-1]:],\n",
        "        skip_special_tokens=True,\n",
        "    )\n",
        "    return {\"response\": result}\n",
        "\n",
        "@app.post(\"/summarize-rag\")\n",
        "def summarize_rag(prompt: Prompt):\n",
        "    summary_topic = prompt.text if prompt.text else \"the provided documents\"\n",
        "    retrieval_query = f\"Information about {summary_topic}\"\n",
        "\n",
        "    retrieved_docs = retrieve_documents(retrieval_query)\n",
        "\n",
        "    if retrieved_docs:\n",
        "        context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": f\"Based on the following information:\\n\\n{context}\\n\\nSummarize this information in a concise way.\"}\n",
        "        ]\n",
        "    else:\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": f\"I could not find relevant information to summarize about {summary_topic}.\"}\n",
        "        ]\n",
        "\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(model.device)\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        outputs = model.generate(inputs, max_new_tokens=150) # Increased max_new_tokens for summaries\n",
        "\n",
        "    result = tokenizer.decode(\n",
        "        outputs[0][inputs.shape[-1]:],\n",
        "        skip_special_tokens=True,\n",
        "    )\n",
        "    return {\"summary\": result}\n",
        "\n",
        "@app.post(\"/generate\")\n",
        "def generate(prompt: Prompt):\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": prompt.text}\n",
        "    ]\n",
        "\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(model.device)\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        outputs = model.generate(inputs, max_new_tokens=80)\n",
        "\n",
        "    result = tokenizer.decode(\n",
        "        outputs[0][inputs.shape[-1]:],\n",
        "        skip_special_tokens=True,\n",
        "    )\n",
        "    return {\"response\": result}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/llm_api/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56d1bd3d"
      },
      "source": [
        "**Reasoning**:\n",
        "The `app.py` file has been updated with the new RAG endpoints. Now I will display the first 100 lines of the file to verify that the changes have been applied correctly, especially the new endpoints and imports.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93e54aa3",
        "outputId": "d88e081e-3636-4534-e28e-1f73b89c0965"
      },
      "source": [
        "%%bash\n",
        "sed -n '1,150p' /content/llm_api/app.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "from fastapi import FastAPI, UploadFile, File\n",
            "from pydantic import BaseModel\n",
            "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
            "import torch\n",
            "from langchain_community.document_loaders import TextLoader\n",
            "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
            "from sentence_transformers import SentenceTransformer\n",
            "from langchain_community.vectorstores import FAISS\n",
            "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
            "import os\n",
            "import shutil\n",
            "\n",
            "MODEL_NAME = \"google/gemma-3-1b-it\"\n",
            "EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
            "FAISS_INDEX_PATH = \"faiss_index\"\n",
            "\n",
            "app = FastAPI()\n",
            "\n",
            "tokenizer = None\n",
            "model = None\n",
            "embedding_model_langchain = None\n",
            "vector_store = None\n",
            "\n",
            "class Prompt(BaseModel):\n",
            "    text: str\n",
            "\n",
            "@app.get(\"/\")\n",
            "def root():\n",
            "    return {\"status\": \"ok\", \"message\": \"LLM API is running\"}\n",
            "\n",
            "@app.on_event(\"startup\")\n",
            "def load_model():\n",
            "    global tokenizer, model, embedding_model_langchain, vector_store\n",
            "    print(\"🔄 Loading models...\")\n",
            "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
            "    model = AutoModelForCausalLM.from_pretrained(\n",
            "        MODEL_NAME,\n",
            "        dtype=torch.float16,\n",
            "        device_map=\"auto\",\n",
            "    ).eval()\n",
            "\n",
            "    embedding_model_langchain = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={'device': 'cuda'})\n",
            "    print(\"✅ Models loaded\")\n",
            "\n",
            "    load_vector_store()\n",
            "\n",
            "def load_and_chunk_document(file_path: str):\n",
            "    loader = TextLoader(file_path)\n",
            "    documents = loader.load()\n",
            "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
            "    chunks = text_splitter.split_documents(documents)\n",
            "    return chunks\n",
            "\n",
            "def create_and_save_vector_store(chunks):\n",
            "    global vector_store\n",
            "    print(\"Building and saving FAISS index...\")\n",
            "    vector_store = FAISS.from_documents(chunks, embedding_model_langchain)\n",
            "    vector_store.save_local(FAISS_INDEX_PATH)\n",
            "    print(f\"FAISS index saved to {FAISS_INDEX_PATH}\")\n",
            "\n",
            "def load_vector_store():\n",
            "    global vector_store\n",
            "    if os.path.exists(FAISS_INDEX_PATH):\n",
            "        print(f\"Loading FAISS index from {FAISS_INDEX_PATH}...\")\n",
            "        vector_store = FAISS.load_local(FAISS_INDEX_PATH, embedding_model_langchain, allow_dangerous_deserialization=True)\n",
            "        print(\"FAISS index loaded.\")\n",
            "    else:\n",
            "        print(\"No existing FAISS index found. A new one will be created if needed.\")\n",
            "\n",
            "def retrieve_documents(query: str, k: int = 4):\n",
            "    if vector_store is None:\n",
            "        print(\"Vector store not initialized. Cannot retrieve documents.\")\n",
            "        return []\n",
            "    print(f\"Retrieving top {k} documents for query: '{query}'\")\n",
            "    docs = vector_store.similarity_search(query, k=k)\n",
            "    return docs\n",
            "\n",
            "@app.post(\"/upload-document\")\n",
            "async def upload_document(file: UploadFile = File(...)):\n",
            "    try:\n",
            "        # Create a temporary file to store the uploaded content\n",
            "        temp_file_path = f\"/tmp/{file.filename}\"\n",
            "        with open(temp_file_path, \"wb\") as buffer:\n",
            "            shutil.copyfileobj(file.file, buffer)\n",
            "\n",
            "        # Process the document and create the vector store\n",
            "        chunks = load_and_chunk_document(temp_file_path)\n",
            "        create_and_save_vector_store(chunks)\n",
            "\n",
            "        return {\"message\": f\"Document '{file.filename}' processed and vector store built successfully.\"}\n",
            "    except Exception as e:\n",
            "        return {\"error\": str(e)}\n",
            "    finally:\n",
            "        # Clean up the temporary file\n",
            "        if os.path.exists(temp_file_path):\n",
            "            os.remove(temp_file_path)\n",
            "\n",
            "@app.post(\"/ask-rag\")\n",
            "def ask_rag(prompt: Prompt):\n",
            "    user_query = prompt.text\n",
            "    retrieved_docs = retrieve_documents(user_query)\n",
            "    \n",
            "    if retrieved_docs:\n",
            "        context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
            "        messages = [\n",
            "            {\"role\": \"user\", \"content\": f\"Based on the following information:\\n\\n{context}\\n\\nAnswer the question: {user_query}\"}\n",
            "        ]\n",
            "    else:\n",
            "        messages = [\n",
            "            {\"role\": \"user\", \"content\": f\"I could not find relevant information in the knowledge base. Please try rephrasing or asking a general question: {user_query}\"}\n",
            "        ]\n",
            "\n",
            "    inputs = tokenizer.apply_chat_template(\n",
            "        messages,\n",
            "        add_generation_prompt=True,\n",
            "        return_tensors=\"pt\",\n",
            "    ).to(model.device)\n",
            "\n",
            "    with torch.inference_mode():\n",
            "        outputs = model.generate(inputs, max_new_tokens=80)\n",
            "\n",
            "    result = tokenizer.decode(\n",
            "        outputs[0][inputs.shape[-1]:],\n",
            "        skip_special_tokens=True,\n",
            "    )\n",
            "    return {\"response\": result}\n",
            "\n",
            "@app.post(\"/summarize-rag\")\n",
            "def summarize_rag(prompt: Prompt):\n",
            "    summary_topic = prompt.text if prompt.text else \"the provided documents\"\n",
            "    retrieval_query = f\"Information about {summary_topic}\"\n",
            "    \n",
            "    retrieved_docs = retrieve_documents(retrieval_query)\n",
            "\n",
            "    if retrieved_docs:\n",
            "        context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
            "        messages = [\n",
            "            {\"role\": \"user\", \"content\": f\"Based on the following information:\\n\\n{context}\\n\\nSummarize this information in a concise way.\"}\n",
            "        ]\n",
            "    else:\n",
            "        messages = [\n",
            "            {\"role\": \"user\", \"content\": f\"I could not find relevant information to summarize about {summary_topic}.\"}\n",
            "        ]\n",
            "    \n",
            "    inputs = tokenizer.apply_chat_template(\n",
            "        messages,\n",
            "        add_generation_prompt=True,\n",
            "        return_tensors=\"pt\",\n",
            "    ).to(model.device)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a61d51a4"
      },
      "source": [
        "## Test RAG Functionality\n",
        "\n",
        "### Subtask:\n",
        "Thoroughly test all new RAG-related FastAPI endpoints (document upload, summarization, and question answering) to ensure they are working as expected and providing accurate results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adf67e72"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step in testing the RAG functionality is to stop the currently running Uvicorn server to ensure a clean restart with the updated `app.py`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aebe84ec"
      },
      "source": [
        "!kill $(lsof -t -i:8000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b9676d6"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the previous Uvicorn server has been stopped, I need to restart it to load the updated `app.py` with the new RAG endpoints.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c83b487"
      },
      "source": [
        "!nohup python -m uvicorn llm_api.app:app --host 0.0.0.0 --port 8000 > uvicorn.log 2>&1 &"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e568fbb"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the previous Uvicorn server has been stopped, I need to restart it to load the updated `app.py` with the new RAG endpoints.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79eb0127"
      },
      "source": [
        "!nohup python -m uvicorn llm_api.app:app --host 0.0.0.0 --port 8000 > uvicorn.log 2>&1 &"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f91f03b0"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the previous Uvicorn server has been stopped, I need to restart it to load the updated `app.py` with the new RAG endpoints.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bf2e19b"
      },
      "source": [
        "import time\n",
        "time.sleep(5) # Give the server a moment to shut down cleanly before restarting\n",
        "!nohup python -m uvicorn llm_api.app:app --host 0.0.0.0 --port 8000 > uvicorn.log 2>&1 &"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4e475d1"
      },
      "source": [
        "**Reasoning**:\n",
        "As instructed, I will create a dummy text file named `test_document.txt` with sample content in the `/content/llm_api/` directory to be used for testing the RAG functionalities.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94764c24",
        "outputId": "9f3b24ef-cc2c-460d-8923-36f53257b78d"
      },
      "source": [
        "%%writefile /content/llm_api/test_document.txt\n",
        "Jupiter is the largest planet in our solar system. It is a gas giant with a mass more than two and a half times that of all the other planets in the Solar System combined, but slightly less than one-thousandth the mass of the Sun. Jupiter is the fifth planet from the Sun. It is primarily composed of hydrogen and helium. It has a Great Red Spot, which is a giant storm larger than Earth. Jupiter has 79 known moons, including the four largest: Io, Europa, Ganymede, and Callisto, often referred to as the Galilean moons."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/llm_api/test_document.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8eba67f"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to upload the `test_document.txt` file to the `/upload-document` endpoint using `requests.post`. I will ensure the file is sent correctly as `multipart/form-data` as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "cff51672",
        "outputId": "e731de6b-7027-418a-b93e-8201a804e7d6"
      },
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "file_path = \"/content/llm_api/test_document.txt\"\n",
        "\n",
        "with open(file_path, \"rb\") as f:\n",
        "    files = {'file': (os.path.basename(file_path), f, 'text/plain')}\n",
        "    response = requests.post(\"http://localhost:8000/upload-document\", files=files)\n",
        "\n",
        "print(response.json())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ConnectionError",
          "evalue": "HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /upload-document (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7eaa71c0f7d0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             sock = connection.create_connection(\n\u001b[0m\u001b[1;32m    199\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;31m# Break explicitly a reference cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             conn.request(\n\u001b[0m\u001b[1;32m    494\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputheader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1332\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1333\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1092\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1036\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tunnel_host\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             raise NewConnectionError(\n\u001b[0m\u001b[1;32m    214\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Failed to establish a new connection: {e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x7eaa71c0f7d0>: Failed to establish a new connection: [Errno 111] Connection refused",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             retries = retries.increment(\n\u001b[0m\u001b[1;32m    842\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mreason\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /upload-document (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7eaa71c0f7d0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1721514976.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text/plain'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"http://localhost:8000/upload-document\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \"\"\"\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /upload-document (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7eaa71c0f7d0>: Failed to establish a new connection: [Errno 111] Connection refused'))"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44c52cdc"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to upload the document failed with a `ConnectionError` because the FastAPI server was likely not fully started or accessible. I need to add a sufficient delay after restarting the Uvicorn server to ensure all models are loaded and the server is ready to handle requests before attempting to upload the document.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "422e892a",
        "outputId": "5e02a111-3a60-45b7-811a-5014ca5fed15"
      },
      "source": [
        "import requests\n",
        "import os\n",
        "import time\n",
        "import subprocess # Added import for subprocess\n",
        "\n",
        "# Ensure any previous Uvicorn processes are killed\n",
        "# Use a more robust kill command that doesn't error if no process is found\n",
        "subprocess.run(\"kill -9 $(lsof -t -i:8000) 2>/dev/null || true\", shell=True)\n",
        "\n",
        "# Restart Uvicorn server with the updated app.py\n",
        "uvicorn_log_path = \"/content/uvicorn.log\"\n",
        "# Clear the log file before starting uvicorn to avoid reading old logs\n",
        "open(uvicorn_log_path, 'w').close()\n",
        "# Use subprocess.Popen for non-blocking execution of uvicorn\n",
        "subprocess.Popen(f\"nohup python -m uvicorn llm_api.app:app --host 0.0.0.0 --port 8000 > {uvicorn_log_path} 2>&1 &\", shell=True)\n",
        "\n",
        "# Wait for the server to start up and models to load (polling log file)\n",
        "print(\"Waiting for FastAPI server to start and models to load...\")\n",
        "server_and_models_ready = False\n",
        "start_time = time.time()\n",
        "timeout = 240 # Increased timeout to 4 minutes for model loading\n",
        "\n",
        "while time.time() - start_time < timeout:\n",
        "    if os.path.exists(uvicorn_log_path):\n",
        "        with open(uvicorn_log_path, 'r') as f:\n",
        "            log_content = f.read()\n",
        "            if \"✅ Models loaded\" in log_content:\n",
        "                server_and_models_ready = True\n",
        "                print(\"FastAPI server and models are ready.\")\n",
        "                break\n",
        "    time.sleep(5) # Check every 5 seconds\n",
        "\n",
        "if not server_and_models_ready:\n",
        "    # Read the log file one last time if not ready, to include it in the error message\n",
        "    final_log_content = \"Log file not found or empty.\" if not os.path.exists(uvicorn_log_path) else open(uvicorn_log_path, 'r').read()\n",
        "    raise Exception(f\"FastAPI server and models did not start in time ({timeout} seconds). Check {uvicorn_log_path} for errors. \\n--- Uvicorn Log Content ---\\n{final_log_content}\\n---------------------------\")\n",
        "\n",
        "file_path = \"/content/llm_api/test_document.txt\"\n",
        "\n",
        "with open(file_path, \"rb\") as f:\n",
        "    files = {'file': (os.path.basename(file_path), f, 'text/plain')}\n",
        "    response = requests.post(\"http://localhost:8000/upload-document\", files=files)\n",
        "\n",
        "print(response.json())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waiting for FastAPI server to start and models to load...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "FastAPI server and models did not start in time (240 seconds). Check /content/uvicorn.log for errors. \n--- Uvicorn Log Content ---\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/__main__.py\", line 4, in <module>\n    uvicorn.main()\n  File \"/usr/local/lib/python3.12/dist-packages/click/core.py\", line 1485, in __call__\n    return self.main(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/click/core.py\", line 1406, in main\n    rv = self.invoke(ctx)\n         ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/click/core.py\", line 1269, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/click/core.py\", line 824, in invoke\n    return callback(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/main.py\", line 423, in main\n    run(\n  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/main.py\", line 593, in run\n    server.run()\n  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 67, in run\n    return asyncio_run(self.serve(sockets=sockets), loop_factory=self.config.get_loop_factory())\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 195, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 691, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 71, in serve\n    await self._serve(sockets)\n  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 78, in _serve\n    config.load()\n  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/config.py\", line 439, in load\n    self.loaded_app = import_from_string(self.app)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/importer.py\", line 22, in import_from_string\n    raise exc from None\n  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/importer.py\", line 19, in import_from_string\n    module = importlib.import_module(module_str)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 999, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n  File \"/content/llm_api/app.py\", line 5, in <module>\n    from langchain_community.document_loaders import TextLoader\nModuleNotFoundError: No module named 'langchain_community'\n\n---------------------------",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3091100168.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Read the log file one last time if not ready, to include it in the error message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mfinal_log_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Log file not found or empty.\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muvicorn_log_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muvicorn_log_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"FastAPI server and models did not start in time ({timeout} seconds). Check {uvicorn_log_path} for errors. \\n--- Uvicorn Log Content ---\\n{final_log_content}\\n---------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/llm_api/test_document.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: FastAPI server and models did not start in time (240 seconds). Check /content/uvicorn.log for errors. \n--- Uvicorn Log Content ---\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/__main__.py\", line 4, in <module>\n    uvicorn.main()\n  File \"/usr/local/lib/python3.12/dist-packages/click/core.py\", line 1485, in __call__\n    return self.main(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/click/core.py\", line 1406, in main\n    rv = self.invoke(ctx)\n         ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/click/core.py\", line 1269, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/click/core.py\", line 824, in invoke\n    return callback(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/main.py\", line 423, in main\n    run(\n  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/main.py\", line 593, in run\n    server.run()\n  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 67, in run\n    return asyncio_run(self.serve(sockets=sockets), loop_factory=self.config.get_loop_factory())\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 195, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 691, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 71, in serve\n    await self._serve(sockets)\n  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 78, in _serve\n    config.load()\n  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/config.py\", line 439, in load\n    self.loaded_app = import_from_string(self.app)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/importer.py\", line 22, in import_from_string\n    raise exc from None\n  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/importer.py\", line 19, in import_from_string\n    module = importlib.import_module(module_str)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 999, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n  File \"/content/llm_api/app.py\", line 5, in <module>\n    from langchain_community.document_loaders import TextLoader\nModuleNotFoundError: No module named 'langchain_community'\n\n---------------------------"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9df4f73"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous error `ModuleNotFoundError: No module named 'langchain_community'` indicates that the `langchain-community` package, which provides `TextLoader`, is missing. Although `langchain` was installed, its sub-packages might not be correctly recognized or installed. Explicitly installing `langchain-community` will resolve this dependency issue.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6066786c",
        "outputId": "d3f4f3f4-621d-47cd-e6a4-7be865408aa9"
      },
      "source": [
        "pip install langchain-community"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.2.1)\n",
            "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community)\n",
            "  Downloading langchain_classic-1.0.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.45)\n",
            "Collecting requests<3.0.0,>=2.32.5 (from langchain-community)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.59)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting langchain-core<2.0.0,>=1.0.1 (from langchain-community)\n",
            "  Downloading langchain_core-1.2.7-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting langchain-text-splitters<2.0.0,>=1.1.0 (from langchain-classic<2.0.0,>=1.0.0->langchain-community)\n",
            "  Downloading langchain_text_splitters-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.12.3)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (25.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (0.12.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.11.12)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.12.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.1->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.41.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_classic-1.0.1-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-1.2.7-py3-none-any.whl (490 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m490.2/490.2 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-1.1.0-py3-none-any.whl (34 kB)\n",
            "Downloading marshmallow-3.26.2-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: requests, mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-core, langchain-text-splitters, langchain-classic, langchain-community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 1.2.1\n",
            "    Uninstalling langchain-core-1.2.1:\n",
            "      Successfully uninstalled langchain-core-1.2.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 langchain-classic-1.0.1 langchain-community-0.4.1 langchain-core-1.2.7 langchain-text-splitters-1.1.0 marshmallow-3.26.2 mypy-extensions-1.1.0 requests-2.32.5 typing-inspect-0.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "requests"
                ]
              },
              "id": "9f9d60e757454efa85aeba2eda6ba546"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b989a1b"
      },
      "source": [
        "**Reasoning**:\n",
        "The `langchain-community` package has now been installed, which should resolve the `ModuleNotFoundError`. I will now re-attempt to restart the FastAPI server and upload the document, using the robust polling mechanism to ensure the server and models are fully loaded before proceeding.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24c09bbc"
      },
      "source": [
        "import requests\n",
        "import os\n",
        "import time\n",
        "import subprocess # Added import for subprocess\n",
        "\n",
        "# Ensure any previous Uvicorn processes are killed\n",
        "# Use a more robust kill command that doesn't error if no process is found\n",
        "subprocess.run(\"kill -9 $(lsof -t -i:8000) 2>/dev/null || true\", shell=True)\n",
        "\n",
        "# Restart Uvicorn server with the updated app.py\n",
        "uvicorn_log_path = \"/content/uvicorn.log\"\n",
        "# Clear the log file before starting uvicorn to avoid reading old logs\n",
        "open(uvicorn_log_path, 'w').close()\n",
        "# Use subprocess.Popen for non-blocking execution of uvicorn\n",
        "subprocess.Popen(f\"nohup python -m uvicorn llm_api.app:app --host 0.0.0.0 --port 8000 > {uvicorn_log_path} 2>&1 &\", shell=True)\n",
        "\n",
        "# Wait for the server to start up and models to load (polling log file)\n",
        "print(\"Waiting for FastAPI server to start and models to load...\")\n",
        "server_and_models_ready = False\n",
        "start_time = time.time()\n",
        "timeout = 240 # Increased timeout to 4 minutes for model loading\n",
        "\n",
        "while time.time() - start_time < timeout:\n",
        "    if os.path.exists(uvicorn_log_path):\n",
        "        with open(uvicorn_log_path, 'r') as f:\n",
        "            log_content = f.read()\n",
        "            if \"✅ Models loaded\" in log_content:\n",
        "                server_and_models_ready = True\n",
        "                print(\"FastAPI server and models are ready.\")\n",
        "                break\n",
        "    time.sleep(5) # Check every 5 seconds\n",
        "\n",
        "if not server_and_models_ready:\n",
        "    # Read the log file one last time if not ready, to include it in the error message\n",
        "    final_log_content = \"Log file not found or empty.\" if not os.path.exists(uvicorn_log_path) else open(uvicorn_log_path, 'r').read()\n",
        "    raise Exception(f\"FastAPI server and models did not start in time ({timeout} seconds). Check {uvicorn_log_path} for errors. \\n--- Uvicorn Log Content ---\\n{final_log_content}\\n---------------------------\")\n",
        "\n",
        "file_path = \"/content/llm_api/test_document.txt\"\n",
        "\n",
        "with open(file_path, \"rb\") as f:\n",
        "    files = {'file': (os.path.basename(file_path), f, 'text/plain')}\n",
        "    response = requests.post(\"http://localhost:8000/upload-document\", files=files)\n",
        "\n",
        "print(response.json())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f52e1aa"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the server is confirmed to be running and the document has been uploaded, I will test the `/ask-rag` endpoint by sending a POST request with a question related to the content of `test_document.txt` to verify the RAG functionality.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "3c460184",
        "outputId": "fc0faf6d-5c47-4e8d-fe6c-78664a34a9ac"
      },
      "source": [
        "import requests\n",
        "import os\n",
        "import time\n",
        "import subprocess\n",
        "\n",
        "# Ensure any previous Uvicorn processes are killed\n",
        "subprocess.run(\"kill -9 $(lsof -t -i:8000) 2>/dev/null || true\", shell=True)\n",
        "\n",
        "# Change directory to where app.py is located\n",
        "os.chdir('/content/llm_api')\n",
        "\n",
        "# Restart Uvicorn server with the updated app.py\n",
        "uvicorn_log_path = \"uvicorn.log\" # Log file will be in /content/llm_api\n",
        "# Clear the log file before starting uvicorn to avoid reading old logs\n",
        "open(uvicorn_log_path, 'w').close()\n",
        "# Use subprocess.Popen for non-blocking execution of uvicorn\n",
        "subprocess.Popen(f\"nohup python -m uvicorn app:app --host 0.0.0.0 --port 8000 > {uvicorn_log_path} 2>&1 &\", shell=True)\n",
        "\n",
        "# Wait for the server to start up and models to load (polling log file)\n",
        "print(\"Waiting for FastAPI server to start and models to load...\")\n",
        "server_and_models_ready = False\n",
        "start_time = time.time()\n",
        "timeout = 300 # Increased timeout to 5 minutes for model loading\n",
        "\n",
        "while time.time() - start_time < timeout:\n",
        "    if os.path.exists(uvicorn_log_path):\n",
        "        with open(uvicorn_log_path, 'r') as f:\n",
        "            log_content = f.read()\n",
        "            if \"✅ Models loaded\" in log_content:\n",
        "                server_and_models_ready = True\n",
        "                print(\"FastAPI server and models are ready.\")\n",
        "                break\n",
        "    time.sleep(5) # Check every 5 seconds\n",
        "\n",
        "if not server_and_models_ready:\n",
        "    # Read the log file one last time if not ready, to include it in the error message\n",
        "    final_log_content = \"Log file not found or empty.\" if not os.path.exists(uvicorn_log_path) else open(uvicorn_log_path, 'r').read()\n",
        "    raise Exception(f\"FastAPI server and models did not start in time ({timeout} seconds). Check {uvicorn_log_path} for errors. \\n--- Uvicorn Log Content ---\\n{final_log_content}\\n---------------------------\")\n",
        "\n",
        "# Create a dummy text file named test_document.txt\n",
        "document_content = \"\"\"\n",
        "Jupiter is the largest planet in our solar system. It is a gas giant with a mass more than two and a half times that of all the other planets in the Solar System combined, but slightly less than one-thousandth the mass of the Sun. Jupiter is the fifth planet from the Sun. It is primarily composed of hydrogen and helium. It has a Great Red Spot, which is a giant storm larger than Earth. Jupiter has 79 known moons, including the four largest: Io, Europa, Ganymede, and Callisto, often referred to as the Galilean moons.\n",
        "\"\"\"\n",
        "with open(\"test_document.txt\", \"w\") as f:\n",
        "    f.write(document_content)\n",
        "\n",
        "# Upload the test_document.txt file\n",
        "file_path = \"test_document.txt\"\n",
        "\n",
        "print(\"Uploading document...\")\n",
        "with open(file_path, \"rb\") as f:\n",
        "    files = {'file': (os.path.basename(file_path), f, 'text/plain')}\n",
        "    response = requests.post(\"http://localhost:8000/upload-document\", files=files)\n",
        "\n",
        "print(f\"Upload Response: {response.json()}\")\n",
        "\n",
        "# Wait a bit for the vector store to be built and saved after upload\n",
        "time.sleep(10)\n",
        "\n",
        "# Test the /ask-rag endpoint\n",
        "rag_question = \"What is the largest planet in our solar system?\"\n",
        "response_rag = requests.post(\n",
        "    \"http://localhost:8000/ask-rag\",\n",
        "    json={\"text\": rag_question}\n",
        ")\n",
        "print(f\"RAG Question: {rag_question}\")\n",
        "print(f\"RAG Response: {response_rag.json()}\")\n",
        "\n",
        "# Test the /summarize-rag endpoint\n",
        "summary_request = \"Jupiter's moons\"\n",
        "response_summary = requests.post(\n",
        "    \"http://localhost:8000/summarize-rag\",\n",
        "    json={\"text\": summary_request}\n",
        ")\n",
        "print(f\"Summarize Request: {summary_request}\")\n",
        "print(f\"Summary Response: {response_summary.json()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/llm_api'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2466910543.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Change directory to where app.py is located\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/llm_api'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Restart Uvicorn server with the updated app.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/llm_api'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b621a25b"
      },
      "source": [
        "# Task\n",
        "Successfully set up and test the RAG system, including the FastAPI server, document processing (creating and uploading `test_document.txt`), and vector store creation and interaction (asking RAG questions and requesting summaries)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "106c649f"
      },
      "source": [
        "## Ensure llm_api directory exists\n",
        "\n",
        "### Subtask:\n",
        "Add code to explicitly create the `/content/llm_api` directory if it does not already exist, before attempting to change the current working directory. This will prevent `FileNotFoundError`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76df46c6",
        "outputId": "7fd84e73-3d1f-452b-c418-5501bd2d3100"
      },
      "source": [
        "import requests\n",
        "import os\n",
        "import time\n",
        "import subprocess\n",
        "\n",
        "# Ensure the /content/llm_api directory exists\n",
        "if not os.path.exists('/content/llm_api'):\n",
        "    os.makedirs('/content/llm_api')\n",
        "\n",
        "# Ensure any previous Uvicorn processes are killed\n",
        "subprocess.run(\"kill -9 $(lsof -t -i:8000) 2>/dev/null || true\", shell=True)\n",
        "\n",
        "# Change directory to where app.py is located\n",
        "os.chdir('/content/llm_api')\n",
        "\n",
        "# Restart Uvicorn server with the updated app.py\n",
        "uvicorn_log_path = \"uvicorn.log\" # Log file will be in /content/llm_api\n",
        "# Clear the log file before starting uvicorn to avoid reading old logs\n",
        "open(uvicorn_log_path, 'w').close()\n",
        "# Use subprocess.Popen for non-blocking execution of uvicorn\n",
        "subprocess.Popen(f\"nohup python -m uvicorn app:app --host 0.0.0.0 --port 8000 > {uvicorn_log_path} 2>&1 &\", shell=True)\n",
        "\n",
        "# Wait for the server to start up and models to load (polling log file)\n",
        "print(\"Waiting for FastAPI server to start and models to load...\")\n",
        "server_and_models_ready = False\n",
        "start_time = time.time()\n",
        "timeout = 300 # Increased timeout to 5 minutes for model loading\n",
        "\n",
        "while time.time() - start_time < timeout:\n",
        "    if os.path.exists(uvicorn_log_path):\n",
        "        with open(uvicorn_log_path, 'r') as f:\n",
        "            log_content = f.read()\n",
        "            if \"✅ Models loaded\" in log_content:\n",
        "                server_and_models_ready = True\n",
        "                print(\"FastAPI server and models are ready.\")\n",
        "                break\n",
        "    time.sleep(5) # Check every 5 seconds\n",
        "\n",
        "if not server_and_models_ready:\n",
        "    # Read the log file one last time if not ready, to include it in the error message\n",
        "    final_log_content = \"Log file not found or empty.\" if not os.path.exists(uvicorn_log_path) else open(uvicorn_log_path, 'r').read()\n",
        "    raise Exception(f\"FastAPI server and models did not start in time ({timeout} seconds). Check {uvicorn_log_path} for errors. \\n--- Uvicorn Log Content ---\\n{final_log_content}\\n---------------------------\")\n",
        "\n",
        "# Create a dummy text file named test_document.txt\n",
        "document_content = \"\"\"\n",
        "Jupiter is the largest planet in our solar system. It is a gas giant with a mass more than two and a half times that of all the other planets in the Solar System combined, but slightly less than one-thousandth the mass of the Sun. Jupiter is the fifth planet from the Sun. It is primarily composed of hydrogen and helium. It has a Great Red Spot, which is a giant storm larger than Earth. Jupiter has 79 known moons, including the four largest: Io, Europa, Ganymede, and Callisto, often referred to as the Galilean moons.\n",
        "\"\"\"\n",
        "with open(\"test_document.txt\", \"w\") as f:\n",
        "    f.write(document_content)\n",
        "\n",
        "# Upload the test_document.txt file\n",
        "file_path = \"test_document.txt\"\n",
        "\n",
        "print(\"Uploading document...\")\n",
        "with open(file_path, \"rb\") as f:\n",
        "    files = {'file': (os.path.basename(file_path), f, 'text/plain')}\n",
        "    response = requests.post(\"http://localhost:8000/upload-document\", files=files)\n",
        "\n",
        "print(f\"Upload Response: {response.json()}\")\n",
        "\n",
        "# Wait a bit for the vector store to be built and saved after upload\n",
        "time.sleep(10)\n",
        "\n",
        "# Test the /ask-rag endpoint\n",
        "rag_question = \"What is the largest planet in our solar system?\"\n",
        "response_rag = requests.post(\n",
        "    \"http://localhost:8000/ask-rag\",\n",
        "    json={\"text\": rag_question}\n",
        ")\n",
        "print(f\"RAG Question: {rag_question}\")\n",
        "print(f\"RAG Response: {response_rag.json()}\")\n",
        "\n",
        "# Test the /summarize-rag endpoint\n",
        "summary_request = \"Jupiter's moons\"\n",
        "response_summary = requests.post(\n",
        "    \"http://localhost:8000/summarize-rag\",\n",
        "    json={\"text\": summary_request}\n",
        ")\n",
        "print(f\"Summarize Request: {summary_request}\")\n",
        "print(f\"Summary Response: {response_summary.json()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waiting for FastAPI server to start and models to load...\n"
          ]
        }
      ]
    }
  ]
}